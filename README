My current work involves training multiple classifiers, some with only a few features and some with 10-12. I do not
expect to have classifiers with more than 50 features. Often, these classifiers are trained in parallel. This runs
reasonably well on my laptop when looking at small examples with only a handful of classifiers. However, the same small
examples run very poorly on a machine with a 16 core, AMD Threadripper cpu (1950x). 

The CSVExample.java class found in DeepLearning4J's examples was is close enough to my own code to demonstrate this issue. 
Below are results for running this test on various CPUs, OSes, and ram configurations. The test uses CSVExample.java,
unmodified except for increasing iterations from 1,000 to 5,000. The results seem to indicate that 
higher core count CPUs, regardless of OS, ram configuration, or CPU vendor suffer significant performance penalties.

In order to run this using gradle:
    In the root of this directory:
        gradle -PmainClass=Runner execute

NOTE: Lombok version 1.18.4 is required for this build script to work with Java 11. This should be handled as a gradle
import automatically.

The repo should load up as an IntelliJ project if you would prefer to run it that way. 

Training the same classifier 5 times serially:
    Intel 6920HQ, 16 GB ram, MacOS 10.14, 4 cores (8 threads)
        0.9.1:
            Average runtime: 9.754 seconds
            Total runtime: 48.774 seconds
        1.0.0-beta3:
            Average runtime: 4.849 seconds
            Total runtime: 24.246 seconds

    Intel 8700B, 16 GB ram, MacOS 10.14, 6 cores (12 threads)
        0.9.1:
            Average runtime: 11.358 seconds
            Total runtime: 56.793 seconds
        1.0.0-beta3:
            Average runtime: 4.478 seconds
            Total runtime: 22.390 seconds

    Intel 7980XE, 32 GB ram, Ubuntu 16.04 LTS, 18 cores (36 threads)
        0.9.1:
            Average runtime: 24.808 seconds
            Total runtime: 119.167 seconds
        1.0.0-beta3:
            Average runtime: 2.148 seconds
            Total runtime: 10.743 seconds

    AMD 1700X, 32 GB ram, Ubuntu 16.04 LTS, 8 cores (16 threads)
        0.9.1: [OpenBLAS]
            Average runtime: 55.383 seconds
            Total runtime: 276.917 seconds 
        1.0.0-beta3: [MKL]
            Average runtime: 98.599 seconds
            Total runtime: 492.995 seconds

    AMD 1950X, 64 GB ram, Windows 10 (whatever the current update is), 16 cores (32 threads)
        0.9.1: [OpenBLAS]
            Average runtime: 85.282 seconds
            Total runtime: 426.413 seconds
        1.0.0-beta3 [MKL]
            Average runtime: 61.491 seconds
            Total runtime: 307.461 seconds

Training the same 5 classifiers in parallel:
    Intel 6920HQ, 16 GB ram, MacOS 10.14, 4 cores (8 threads)
        0.9.1:
            Runtime: 35.926 seconds
        1.0.0-beta3:
            Runtime: 11.590 seconds

    Intel 8700B, 16 GB ram, MacOS 10.14, 6 cores (12 threads)
        0.9.1:
            Runtime: 46.211 seconds
        1.0.0-beta3:
            Runtime: 14.478 seconds

    Intel 7980XE, 32 GB ram, Ubuntu 16.04 LTS, 18 cores (36 threads 
        0.9.1:
            Runtime: 292.449 seconds
        1.0.0-beta3:
            Runtime: 11.417 seconds

    AMD 1700X, 32 GB ram, Ubuntu 16.04 LTS, 8 cores (16 threads)
        0.9.1: [OpenBLAS]
            Runtime: 46.967 seconds
        1.0.0-beta3: [MKL]
            Runtime: 18.109 seconds

    AMD 1950X, 64 GB ram, Windows 10 (whatever the current update is), 16 cores (32 threads)
        0.9.1: [OpenBLAS]
            Runtime: 810.631 seconds
        1.0.0-beta3: [MKL]
            Runtime: 27.291 seconds

I'm lost as far as how to proceed to solve this issue, if somebody sees something in the code that obviously won't work
well for high core count CPUs please point it out, I need to get this resolved. 

Some questions:
    - How is the 1700X training 5 classifiers in parallel faster than it can train a single classifier? 
    - Why is the 1950x so much slower than the 1700x when they both have the same base clock (assuming worst case, no boost) 
    when running serially?
    - Why did the 1950x take significantly longer running in parallel when the 1700x saw a speedup running in parallel?
    - Thread scheduling issue?
    - NUMA issue?
    - Cache thrashing?

Some notes:
    - All CPUs are adaquetly cooled 
    - Where applicable, quad channel memory is enabled 
    - No other major processes running during testing
    - All CPUs and RAM running at default clocks
    - Data is being read from an SSD 
        - 1950X
        - 6920HQ
        - 8700B
    - Data is being read from an HDD 
        - 1700X
        - 7980XE
